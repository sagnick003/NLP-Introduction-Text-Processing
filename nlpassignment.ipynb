{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DS-AG-028\n",
        "\n",
        "NLP Introduction & Text Processing |\n",
        "Assignment\n",
        "\n",
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Answer:Computational Linguistics is an interdisciplinary field that combines linguistics (the scientific study of language) with computer science to enable computers to understand, analyze, and generate human language. It focuses on creating formal models and algorithms that represent linguistic knowledge such as grammar, syntax, semantics, and phonetics so that machines can process natural language in a meaningful way.\n",
        "\n",
        "Computational Linguistics involves studying the structure and rules of language and translating them into computational models. It uses both rule-based approaches (based on linguistic theories) and statistical or machine learning methods to analyze text and speech.\n",
        "\n",
        "Natural Language Processing (NLP) is a practical application of Computational Linguistics. While Computational Linguistics focuses more on the theoretical and scientific understanding of language and developing models, NLP focuses on building real-world systems and applications that use these models. In simple terms, Computational Linguistics provides the foundation, and NLP uses that foundation to create usable technologies.\n",
        "\n",
        "The relationship between Computational Linguistics and NLP can be understood as follows:\n",
        "\n",
        "Foundation vs Application:\n",
        "\n",
        "Computational Linguistics develops language models and theories, while NLP applies them to build applications.\n",
        "\n",
        "Language Understanding:\n",
        "\n",
        "Computational Linguistics helps computers understand linguistic structures, which NLP uses to perform tasks like translation, summarization, and sentiment analysis.\n",
        "\n",
        "Shared Goal:\n",
        "\n",
        "Both aim to enable computers to process and understand human language effectively.\n",
        "\n",
        "Examples of NLP applications based on Computational Linguistics include:\n",
        "\n",
        "Machine translation (e.g., English to Hindi translation)\n",
        "\n",
        "Chatbots and virtual assistants\n",
        "\n",
        "Spell checkers and grammar correction tools\n",
        "\n",
        "Text summarization systems\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "In conclusion, Computational Linguistics is the scientific and theoretical backbone of Natural Language Processing, while NLP is the applied field that uses those theories and models to create intelligent language-based applications.\n",
        "\n",
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "Answer:The historical evolution of Natural Language Processing (NLP) can be divided into several major phases, each marked by different approaches and technological advancements.\n",
        "\n",
        "1. Early Beginnings (1950s–1960s): Rule-Based Systems\n",
        "The origin of NLP dates back to the 1950s, when researchers first explored whether computers could understand human language. One of the earliest examples was the Georgetown-IBM experiment in 1954, which demonstrated automatic translation of Russian sentences into English. During this period, NLP systems were based on hand-written rules created by linguists. These systems relied on grammar rules, dictionaries, and pattern matching to process language. However, they were limited because creating rules for every possible language variation was difficult.\n",
        "\n",
        "2. Decline and the ALPAC Report (1966)\n",
        "\n",
        "In 1966, the ALPAC (Automatic Language Processing Advisory Committee) report concluded that machine translation research had not met expectations. As a result, funding for NLP research decreased significantly, and progress slowed during this period.\n",
        "\n",
        "3. Knowledge-Based Systems (1970s–1980s)\n",
        "\n",
        "During this phase, researchers began using knowledge-based approaches. Systems included structured knowledge representations such as semantic networks and ontologies. NLP applications like early chatbots (e.g., ELIZA) and expert systems were developed. These systems attempted to simulate human conversation but still relied heavily on manually created rules.\n",
        "\n",
        "4. Statistical Approaches (1980s–1990s)\n",
        "\n",
        "With the availability of larger digital text datasets and increased computing power, NLP shifted toward statistical methods. Instead of relying only on rules, systems learned patterns from data using probability and statistics. Techniques such as Hidden Markov Models (HMM), probabilistic parsing, and n-grams were widely used. This phase improved tasks like speech recognition and part-of-speech tagging.\n",
        "\n",
        "5. Machine Learning Era (2000s–2010s)\n",
        "\n",
        "Machine learning algorithms such as Naive Bayes, Support Vector Machines (SVM), and decision trees became popular. These models learned from labeled data and improved performance in tasks like spam detection, sentiment analysis, and text classification. Feature engineering became an important part of NLP.\n",
        "\n",
        "6. Deep Learning Era (2010s–Present)\n",
        "\n",
        "In recent years, deep learning has revolutionized NLP. Neural networks such as Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and Transformers have significantly improved language understanding. Modern models like BERT, GPT, and other transformer-based architectures can understand context, generate human-like text, and perform complex tasks such as translation, summarization, and conversational AI.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The evolution of NLP has progressed from rule-based systems to statistical methods, then to machine learning, and finally to deep learning and transformer-based models. Each stage has improved the ability of computers to understand and process human language, making NLP a powerful technology used in many modern applications.\n",
        "\n",
        "Question 3: List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "Answer:Natural Language Processing (NLP) is widely used in the modern tech industry to enable machines to understand and interact with human language. Three major use cases of NLP are:\n",
        "\n",
        "1.Chatbots and Virtual Assistants\n",
        "\n",
        "NLP is used to build chatbots and virtual assistants that can communicate with users in natural language. These systems understand user queries, interpret their meaning, and provide appropriate responses. They are commonly used in customer support, websites, and mobile applications to answer questions, solve problems, and provide information without human intervention. Examples include customer service chatbots and assistants like Siri, Alexa, and Google Assistant. This helps companies provide 24/7 support and improves user experience.\n",
        "\n",
        "2.Sentiment Analysis\n",
        "\n",
        "Sentiment analysis uses NLP to identify and analyze emotions, opinions, or attitudes expressed in text. It determines whether the sentiment is positive, negative, or neutral. Companies use sentiment analysis to analyze customer reviews, social media comments, and feedback to understand customer satisfaction and improve their products or services. For example, businesses analyze product reviews to identify strengths and weaknesses and make better decisions.\n",
        "\n",
        "3.Machine Translation\n",
        "\n",
        "Machine translation uses NLP to automatically translate text from one language to another. It helps break language barriers and enables global communication. Popular examples include Google Translate and translation features in websites and apps. This use case is important for international businesses, travel, education, and communication, as it allows users to access information in different languages easily.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "These use cases show how NLP helps automate communication, analyze customer opinions, and enable multilingual interaction. As technology advances, NLP continues to play a crucial role in improving user experience and making systems more intelligent and efficient.\n",
        "\n",
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Answer:Text normalization is the process of converting raw text into a standard, consistent, and clean format so that it can be easily understood and processed by computers. It involves transforming text into a uniform form by removing unwanted variations and simplifying the text without changing its original meaning.\n",
        "\n",
        "In natural language processing, text data often contains inconsistencies such as uppercase and lowercase letters, punctuation, numbers, special characters, abbreviations, and spelling variations. Text normalization helps to reduce these inconsistencies and makes the text more suitable for analysis.\n",
        "\n",
        "Common text normalization techniques include:\n",
        "\n",
        "Lowercasing:\n",
        "\n",
        "Converting all text to lowercase so that words like “Apple” and “apple” are treated as the same word.\n",
        "\n",
        "Removing punctuation:\n",
        "\n",
        "Eliminating punctuation marks such as commas, periods, and symbols that may not be useful for analysis.\n",
        "\n",
        "Removing special characters and numbers:\n",
        "\n",
        "Cleaning unnecessary symbols, emojis, or numbers if they are not relevant to the task.\n",
        "\n",
        "Expanding contractions:\n",
        "\n",
        "Converting contractions like “don’t” into “do not” and “can’t” into “cannot”.\n",
        "\n",
        "Removing stop words:\n",
        "\n",
        "Eliminating common words such as “is”, “the”, and “and” that may not add significant meaning.\n",
        "\n",
        "Stemming and lemmatization:\n",
        "\n",
        "Reducing words to their base or root form, such as “running” to “run”.\n",
        "\n",
        "Why text normalization is essential:\n",
        "\n",
        "Improves consistency: It ensures that similar words are treated as the same, reducing duplication.\n",
        "\n",
        "Improves accuracy: Machine learning models perform better with clean and consistent data.\n",
        "\n",
        "Reduces complexity: It reduces the number of unique words, making processing faster and more efficient.\n",
        "\n",
        "Enhances analysis: It helps in better feature extraction and improves the performance of NLP tasks like classification, sentiment analysis, and search.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Text normalization is a crucial step in text processing because it cleans and standardizes text data, making it easier for computers to analyze and understand human language accurately and efficiently.\n",
        "\n",
        "Question 5: Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "Answer:Stemming and lemmatization are text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. This helps in simplifying text and treating different forms of a word as the same for analysis. Although both serve a similar purpose, they differ in their approach and accuracy.\n",
        "\n",
        "1.Stemming:\n",
        "\n",
        "Stemming is the process of removing prefixes or suffixes from a word to obtain its root form, called the stem. It uses simple rule-based methods and does not consider the context or grammar of the word. As a result, the stem may not always be a meaningful or correct word.\n",
        "\n",
        "Examples of stemming:\n",
        "\n",
        "running → run\n",
        "\n",
        "played → play\n",
        "\n",
        "studies → studi\n",
        "\n",
        "happiness → happi\n",
        "\n",
        "In the above examples, “studi” and “happi” are not actual words, but they are stems produced by stemming algorithms.\n",
        "\n",
        "Characteristics of stemming:\n",
        "\n",
        "Faster and simpler\n",
        "\n",
        "Uses rule-based suffix removal\n",
        "\n",
        "May produce incorrect or incomplete words\n",
        "\n",
        "Less accurate\n",
        "\n",
        "2.Lemmatization:\n",
        "\n",
        "Lemmatization is the process of reducing a word to its base or dictionary form, called the lemma. It considers the meaning and grammatical context of the word and uses vocabulary and linguistic analysis. The output is always a valid and meaningful word.\n",
        "\n",
        "Examples of lemmatization:\n",
        "\n",
        "running → run\n",
        "\n",
        "better → good\n",
        "\n",
        "studies → study\n",
        "\n",
        "went → go\n",
        "\n",
        "In these examples, the output words are correct dictionary forms.\n",
        "\n",
        "Characteristics of lemmatization:\n",
        "\n",
        "More accurate than stemming\n",
        "\n",
        "Uses vocabulary and linguistic rules\n",
        "\n",
        "Produces meaningful base words\n",
        "\n",
        "Slower and more complex than stemming\n",
        "\n",
        "3. Comparison between Stemming and Lemmatization:\n",
        "\n",
        "| Feature    | Stemming                               | Lemmatization                           |\n",
        "| ---------- | -------------------------------------- | --------------------------------------- |\n",
        "| Definition | Removes suffix/prefix to get root form | Converts word to dictionary base form   |\n",
        "| Output     | May not be a real word                 | Always a meaningful word                |\n",
        "| Accuracy   | Less accurate                          | More accurate                           |\n",
        "| Speed      | Faster                                 | Slower                                  |\n",
        "| Method     | Rule-based                             | Uses vocabulary and linguistic analysis |\n",
        "| Example    | studies → studi                        | studies → study                         |\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Both stemming and lemmatization help in reducing words to their base form, which improves text processing efficiency. Stemming is faster but less accurate, while lemmatization is slower but produces correct and meaningful words. Lemmatization is generally preferred when accuracy and proper language understanding are important.\n",
        "\n",
        "Question 6: Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "k1AOhN4J8DKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Given text\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regular expression pattern for email addresses\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extracting email addresses using re.findall()\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "# Print extracted emails\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BcmvQ23-BXZ",
        "outputId": "808f5f32-adce-4ffc-bbcb-06f03665fc6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "GVtiaxI5-QQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK\n",
        "!pip install nltk\n",
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, frequency in freq_dist.items():\n",
        "    print(f\"{word} : {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ_BZk3m-eUf",
        "outputId": "7ebd5abd-2d4e-442b-fca1-2d198e3739f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural : 1\n",
            "Language : 1\n",
            "Processing : 1\n",
            "( : 1\n",
            "NLP : 3\n",
            ") : 1\n",
            "is : 2\n",
            "a : 1\n",
            "fascinating : 1\n",
            "field : 1\n",
            "that : 1\n",
            "combines : 1\n",
            "linguistics : 1\n",
            ", : 7\n",
            "computer : 1\n",
            "science : 1\n",
            "and : 3\n",
            "artificial : 1\n",
            "intelligence : 1\n",
            ". : 4\n",
            "It : 1\n",
            "enables : 1\n",
            "machines : 1\n",
            "to : 1\n",
            "understand : 1\n",
            "interpret : 1\n",
            "generate : 1\n",
            "human : 1\n",
            "language : 1\n",
            "Applications : 1\n",
            "of : 2\n",
            "include : 1\n",
            "chatbots : 1\n",
            "sentiment : 1\n",
            "analysis : 1\n",
            "machine : 1\n",
            "translation : 1\n",
            "As : 1\n",
            "technology : 1\n",
            "advances : 1\n",
            "the : 1\n",
            "role : 1\n",
            "in : 1\n",
            "modern : 1\n",
            "solutions : 1\n",
            "becoming : 1\n",
            "increasingly : 1\n",
            "critical : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Tokenization: The paragraph is split into individual words and symbols using word_tokenize().\n",
        "\n",
        "Frequency Distribution: FreqDist() counts how many times each token appears.\n",
        "\n",
        "This helps in analyzing word importance and usage in NLP tasks.\n",
        "\n",
        "Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "KUwai8Rr-2_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy\n",
        "!pip install spacy\n",
        "\n",
        "# Download English language model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"John lives in New York and works at Google. He met Sarah in London last year.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Custom annotator: Identify Proper Nouns\n",
        "print(\"Proper Nouns identified and labeled:\\n\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        print(f\"Word: {token.text} --> Label: Proper Noun\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hfOpcb7_HW3",
        "outputId": "7b6b052c-11fd-4ab6-cf6e-d3ba2240f76e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: typer>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Proper Nouns identified and labeled:\n",
            "\n",
            "Word: John --> Label: Proper Noun\n",
            "Word: New --> Label: Proper Noun\n",
            "Word: York --> Label: Proper Noun\n",
            "Word: Google --> Label: Proper Noun\n",
            "Word: Sarah --> Label: Proper Noun\n",
            "Word: London --> Label: Proper Noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "oO_eZwiS_Rmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gensim nltk\n",
        "\n",
        "# Import libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Given dataset\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "for sentence in dataset:\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]\n",
        "\n",
        "    processed_data.append(tokens)\n",
        "\n",
        "# Print processed data\n",
        "print(\"Tokenized and Preprocessed Data:\")\n",
        "for sentence in processed_data:\n",
        "    print(sentence)\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=processed_data,\n",
        "    vector_size=50,   # Size of word vectors\n",
        "    window=3,         # Context window size\n",
        "    min_count=1,      # Include all words\n",
        "    workers=2         # CPU threads\n",
        ")\n",
        "\n",
        "# Example: Get vector for a word\n",
        "print(\"\\nWord Vector for 'word':\")\n",
        "print(model.wv['word'])\n",
        "\n",
        "# Example: Find similar words\n",
        "print(\"\\nWords similar to 'word':\")\n",
        "print(model.wv.most_similar('word'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1oBPV2__ffW",
        "outputId": "e08a4194-cc63-4344-9273-87fe6b61f57f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Tokenized and Preprocessed Data:\n",
            "['natural', 'language', 'processing', 'enables', 'computers', 'understand', 'human', 'language']\n",
            "['word', 'embeddings', 'type', 'word', 'representation', 'allows', 'words', 'similar', 'meaning', 'similar', 'representation']\n",
            "['word2vec', 'popular', 'word', 'embedding', 'technique', 'used', 'many', 'nlp', 'applications']\n",
            "['text', 'preprocessing', 'critical', 'step', 'training', 'word', 'embeddings']\n",
            "['tokenization', 'normalization', 'help', 'clean', 'raw', 'text', 'modeling']\n",
            "\n",
            "Word Vector for 'word':\n",
            "[-1.07354100e-03  4.69773222e-04  1.02030719e-02  1.80177148e-02\n",
            " -1.86136067e-02 -1.42386081e-02  1.29194465e-02  1.79460458e-02\n",
            " -1.00450935e-02 -7.52970017e-03  1.47546269e-02 -3.06760333e-03\n",
            " -9.07208677e-03  1.31036835e-02 -9.70983319e-03 -3.62110930e-03\n",
            "  5.78249805e-03  2.00221152e-03 -1.65881608e-02 -1.89086050e-02\n",
            "  1.46312593e-02  1.01419240e-02  1.35450875e-02  1.53646979e-03\n",
            "  1.27060441e-02 -6.80157030e-03 -1.90524594e-03  1.15397116e-02\n",
            " -1.50511730e-02 -7.86797330e-03 -1.50170429e-02 -1.86529779e-03\n",
            "  1.90796703e-02 -1.46558210e-02 -4.66114841e-03 -3.88691272e-03\n",
            "  1.61575209e-02 -1.18641062e-02  7.77921668e-05 -9.50995460e-03\n",
            " -1.92011707e-02  1.00064455e-02 -1.75157152e-02 -8.78802035e-03\n",
            " -4.76100831e-05 -5.86683105e-04 -1.53219085e-02  1.92190018e-02\n",
            "  9.97238513e-03  1.84636209e-02]\n",
            "\n",
            "Words similar to 'word':\n",
            "[('many', 0.2703843414783478), ('processing', 0.24077028036117554), ('critical', 0.21090067923069), ('word2vec', 0.18619756400585175), ('tokenization', 0.16753865778446198), ('popular', 0.160696342587471), ('step', 0.1501363068819046), ('understand', 0.14491327106952667), ('raw', 0.13222338259220123), ('similar', 0.12704892456531525)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "Answer:As a data scientist at a fintech startup, analyzing customer reviews using NLP involves several important steps to clean, process, and extract meaningful insights.\n",
        "\n",
        "Steps involved in NLP pipeline:\n",
        "\n",
        "1.Data Collection -\n",
        "\n",
        "Collect customer reviews from sources such as:\n",
        "\n",
        "App reviews\n",
        "\n",
        "Customer support chats\n",
        "\n",
        "Emails\n",
        "\n",
        "Surveys\n",
        "\n",
        "Social media\n",
        "\n",
        "2.Text Cleaning -\n",
        "\n",
        "Clean the text to remove noise:\n",
        "\n",
        "Convert text to lowercase\n",
        "\n",
        "Remove punctuation and special characters\n",
        "\n",
        "Remove stopwords\n",
        "\n",
        "Remove extra spaces\n",
        "\n",
        "3.Tokenization -\n",
        "\n",
        "Split text into individual words (tokens).\n",
        "\n",
        "4.Text Normalization -\n",
        "\n",
        "Apply:\n",
        "\n",
        "Stemming or Lemmatization\n",
        "\n",
        "Remove irrelevant words\n",
        "\n",
        "5.Feature Extraction -\n",
        "\n",
        "Convert text into numerical format using:\n",
        "\n",
        "Bag of Words\n",
        "\n",
        "TF-IDF\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "6.Sentiment Analysis -\n",
        "\n",
        "Determine whether feedback is:\n",
        "\n",
        "Positive\n",
        "\n",
        "Negative\n",
        "\n",
        "Neutral\n",
        "\n",
        "7.Insight Extraction -\n",
        "\n",
        "Identify:\n",
        "\n",
        "Common complaints\n",
        "\n",
        "Frequently mentioned features\n",
        "\n",
        "Customer satisfaction trends.\n",
        "\n"
      ],
      "metadata": {
        "id": "6RI9gXPz_lwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install nltk scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample customer reviews\n",
        "reviews = [\n",
        "    \"The app is very easy to use and secure.\",\n",
        "    \"Customer service is slow and unresponsive.\",\n",
        "    \"I love the fast transaction speed!\",\n",
        "    \"The app crashes frequently and has bugs.\",\n",
        "    \"Excellent experience, very reliable service.\"\n",
        "]\n",
        "\n",
        "# Step 1: Text Cleaning and Tokenization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "cleaned_reviews = []\n",
        "\n",
        "for review in reviews:\n",
        "    review = review.lower()\n",
        "    tokens = word_tokenize(review)\n",
        "\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    cleaned_reviews.append(\" \".join(tokens))\n",
        "\n",
        "print(\"Cleaned Reviews:\")\n",
        "for review in cleaned_reviews:\n",
        "    print(review)\n",
        "\n",
        "# Step 2: Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_reviews)\n",
        "\n",
        "print(\"\\nTF-IDF Feature Names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Step 3: Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"\\nSentiment Analysis Results:\")\n",
        "for review in reviews:\n",
        "    score = sia.polarity_scores(review)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Sentiment Score: {score}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQzfytpOAH--",
        "outputId": "9d15ab39-951d-4c90-a63f-a2263214c96e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Cleaned Reviews:\n",
            "app easy use secure\n",
            "customer service slow unresponsive\n",
            "love fast transaction speed\n",
            "app crashes frequently bugs\n",
            "excellent experience reliable service\n",
            "\n",
            "TF-IDF Feature Names:\n",
            "['app' 'bugs' 'crashes' 'customer' 'easy' 'excellent' 'experience' 'fast'\n",
            " 'frequently' 'love' 'reliable' 'secure' 'service' 'slow' 'speed'\n",
            " 'transaction' 'unresponsive' 'use']\n",
            "\n",
            "Sentiment Analysis Results:\n",
            "Review: The app is very easy to use and secure.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6801}\n",
            "\n",
            "Review: Customer service is slow and unresponsive.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Review: I love the fast transaction speed!\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.6696}\n",
            "\n",
            "Review: The app crashes frequently and has bugs.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Review: Excellent experience, very reliable service.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.519, 'pos': 0.481, 'compound': 0.5719}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}